"""
Exploratory Data Analysis (EDA) Component for Fraud Detection System

Analysis Performed:
1. Basic Data Overview
2. Target Variable Analysis
3. Numerical Features Analysis
4. Categorical Features Analysis
5. Correlation Analysis
6. Fraud Pattern Analysis
7. Outlier Detection
8. Visualization Generation
9. EDA Report Generation
"""

import os
import sys
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass, field
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
import warnings

warnings.filterwarnings('ignore')

from src.entity.config_entity import DataIngestionConfig
from src.entity.artifact_entity import DataIngestionArtifact
from src.logger import logger
from src.exception import FraudDetectionException
from src.utils.common import write_json, create_directories
from src.constants import ARTIFACTS_DIR, TARGET_COLUMN


# ============== EDA CONFIG ==============
@dataclass
class EDAConfig:
    """Configuration for EDA Component"""
    
    eda_dir: str = os.path.join(ARTIFACTS_DIR, "eda")
    plots_dir: str = os.path.join(ARTIFACTS_DIR, "eda", "plots")
    report_file: str = "eda_report.json"
    
    # Analysis Settings
    target_column: str = TARGET_COLUMN
    correlation_threshold: float = 0.7
    outlier_std_threshold: float = 3.0
    
    # Plot Settings
    figure_size: Tuple[int, int] = (12, 8)
    plot_style: str = "whitegrid"
    color_palette: str = "husl"
    
    @property
    def report_path(self) -> str:
        return os.path.join(self.eda_dir, self.report_file)


# ============== EDA ARTIFACT ==============
@dataclass
class EDAArtifact:
    """Artifact generated by EDA Component"""
    
    eda_report_path: str
    plots_dir: str
    
    # Data Overview
    total_records: int = 0
    total_features: int = 0
    numerical_features: List[str] = field(default_factory=list)
    categorical_features: List[str] = field(default_factory=list)
    
    # Target Analysis
    fraud_count: int = 0
    non_fraud_count: int = 0
    fraud_percentage: float = 0.0
    imbalance_ratio: float = 0.0
    
    # Key Insights
    high_correlation_pairs: List[Tuple] = field(default_factory=list)
    outlier_columns: List[str] = field(default_factory=list)
    
    is_completed: bool = True
    message: str = "EDA Completed Successfully"


class DataEDA:
    """
    Exploratory Data Analysis Component for Fraud Detection System.
    
    Responsibilities:
    - Analyze data distributions
    - Generate visualizations
    - Identify patterns in fraud cases
    - Create comprehensive EDA report
    """
    
    def __init__(
        self,
        config: EDAConfig = EDAConfig(),
        data_ingestion_artifact: DataIngestionArtifact = None
    ):
        """
        Initialize EDA Component.
        
        Args:
            config: EDAConfig object
            data_ingestion_artifact: Artifact from data ingestion
        """
        self.config = config
        self.data_ingestion_artifact = data_ingestion_artifact
        self.eda_results: Dict = {}
        
        # Set plot style
        sns.set_style(self.config.plot_style)
        plt.rcParams['figure.figsize'] = self.config.figure_size
        
        logger.info(f"{'='*60}")
        logger.info("Initializing EDA Component")
        logger.info(f"{'='*60}")
    
    def _load_data(self) -> pd.DataFrame:
        """Load training data for EDA."""
        logger.info("Loading training data for EDA...")
        
        df = pd.read_csv(self.data_ingestion_artifact.train_file_path)
        logger.info(f"Data loaded. Shape: {df.shape}")
        
        return df
    
    # ==================== BASIC DATA OVERVIEW ====================
    
    def analyze_data_overview(self, df: pd.DataFrame) -> Dict:
        """
        Generate basic data overview statistics.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with overview statistics
        """
        logger.info("Analyzing data overview...")
        
        # Identify feature types
        numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = df.select_dtypes(include=['object']).columns.tolist()
        
        # Remove target from numerical if present
        if self.config.target_column in numerical_features:
            numerical_features.remove(self.config.target_column)
        
        overview = {
            "shape": {
                "rows": df.shape[0],
                "columns": df.shape[1]
            },
            "features": {
                "total": df.shape[1],
                "numerical": len(numerical_features),
                "categorical": len(categorical_features),
                "numerical_list": numerical_features,
                "categorical_list": categorical_features
            },
            "memory_usage_mb": round(df.memory_usage(deep=True).sum() / 1024**2, 2),
            "missing_values": {
                "total": int(df.isnull().sum().sum()),
                "by_column": df.isnull().sum().to_dict()
            },
            "duplicates": {
                "count": int(df.duplicated().sum()),
                "percentage": round((df.duplicated().sum() / len(df)) * 100, 2)
            }
        }
        
        logger.info(f"Total Records: {overview['shape']['rows']:,}")
        logger.info(f"Total Features: {overview['shape']['columns']}")
        logger.info(f"Numerical Features: {len(numerical_features)}")
        logger.info(f"Categorical Features: {len(categorical_features)}")
        
        self.eda_results['overview'] = overview
        return overview
    
    # ==================== TARGET VARIABLE ANALYSIS ====================
    
    def analyze_target_distribution(self, df: pd.DataFrame) -> Dict:
        """
        Analyze target variable (fraud) distribution.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with target distribution analysis
        """
        logger.info("Analyzing target distribution...")
        
        target = self.config.target_column
        
        # Value counts
        value_counts = df[target].value_counts()
        value_percentages = df[target].value_counts(normalize=True) * 100
        
        fraud_count = int(value_counts.get(1, 0))
        non_fraud_count = int(value_counts.get(0, 0))
        
        # Calculate imbalance ratio
        imbalance_ratio = non_fraud_count / max(fraud_count, 1)
        
        target_analysis = {
            "value_counts": value_counts.to_dict(),
            "percentages": value_percentages.round(2).to_dict(),
            "fraud_count": fraud_count,
            "non_fraud_count": non_fraud_count,
            "fraud_percentage": round(value_percentages.get(1, 0), 2),
            "imbalance_ratio": round(imbalance_ratio, 2),
            "imbalance_category": self._categorize_imbalance(imbalance_ratio)
        }
        
        logger.info(f"Fraud Cases: {fraud_count:,} ({target_analysis['fraud_percentage']}%)")
        logger.info(f"Non-Fraud Cases: {non_fraud_count:,}")
        logger.info(f"Imbalance Ratio: 1:{target_analysis['imbalance_ratio']}")
        logger.info(f"Imbalance Category: {target_analysis['imbalance_category']}")
        
        # Generate plot
        self._plot_target_distribution(df, target)
        
        self.eda_results['target_analysis'] = target_analysis
        return target_analysis
    
    def _categorize_imbalance(self, ratio: float) -> str:
        """Categorize the level of class imbalance."""
        if ratio < 5:
            return "Low Imbalance"
        elif ratio < 20:
            return "Moderate Imbalance"
        elif ratio < 100:
            return "High Imbalance"
        else:
            return "Severe Imbalance"
    
    def _plot_target_distribution(self, df: pd.DataFrame, target: str) -> None:
        """Plot target variable distribution."""
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Count plot
        colors = ['#2ecc71', '#e74c3c']
        ax1 = axes[0]
        counts = df[target].value_counts()
        bars = ax1.bar(['Non-Fraud (0)', 'Fraud (1)'], counts.values, color=colors)
        ax1.set_title('Transaction Count by Class', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Count')
        
        # Add count labels on bars
        for bar, count in zip(bars, counts.values):
            ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 500,
                    f'{count:,}', ha='center', va='bottom', fontsize=11)
        
        # Pie chart
        ax2 = axes[1]
        percentages = df[target].value_counts(normalize=True) * 100
        ax2.pie(percentages.values, labels=['Non-Fraud', 'Fraud'],
                autopct='%1.2f%%', colors=colors, explode=[0, 0.1],
                shadow=True, startangle=90)
        ax2.set_title('Fraud Distribution (%)', fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.config.plots_dir, 'target_distribution.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Target distribution plot saved")
    
    # ==================== NUMERICAL FEATURES ANALYSIS ====================
    
    def analyze_numerical_features(self, df: pd.DataFrame) -> Dict:
        """
        Analyze numerical features.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with numerical analysis
        """
        logger.info("Analyzing numerical features...")
        
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        # Remove target column
        if self.config.target_column in numerical_cols:
            numerical_cols.remove(self.config.target_column)
        
        numerical_analysis = {}
        
        for col in numerical_cols:
            stats_dict = {
                "count": int(df[col].count()),
                "mean": round(float(df[col].mean()), 2),
                "std": round(float(df[col].std()), 2),
                "min": round(float(df[col].min()), 2),
                "25%": round(float(df[col].quantile(0.25)), 2),
                "50%": round(float(df[col].quantile(0.50)), 2),
                "75%": round(float(df[col].quantile(0.75)), 2),
                "max": round(float(df[col].max()), 2),
                "skewness": round(float(df[col].skew()), 2),
                "kurtosis": round(float(df[col].kurtosis()), 2),
                "zeros_count": int((df[col] == 0).sum()),
                "zeros_percentage": round(((df[col] == 0).sum() / len(df)) * 100, 2)
            }
            
            numerical_analysis[col] = stats_dict
        
        # Generate plots
        self._plot_numerical_distributions(df, numerical_cols)
        self._plot_boxplots(df, numerical_cols)
        
        self.eda_results['numerical_analysis'] = numerical_analysis
        
        logger.info(f"Analyzed {len(numerical_cols)} numerical features")
        return numerical_analysis
    
    def _plot_numerical_distributions(self, df: pd.DataFrame, columns: List[str]) -> None:
        """Plot distributions for numerical features."""
        n_cols = min(3, len(columns))
        n_rows = (len(columns) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]
        
        for idx, col in enumerate(columns):
            if idx < len(axes):
                ax = axes[idx]
                
                # Plot histogram with KDE
                sns.histplot(data=df, x=col, hue=self.config.target_column,
                           kde=True, ax=ax, palette=['#2ecc71', '#e74c3c'])
                ax.set_title(f'Distribution of {col}', fontsize=12, fontweight='bold')
                ax.set_xlabel(col)
        
        # Hide empty subplots
        for idx in range(len(columns), len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.config.plots_dir, 'numerical_distributions.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Numerical distributions plot saved")
    
    def _plot_boxplots(self, df: pd.DataFrame, columns: List[str]) -> None:
        """Plot boxplots for numerical features by target."""
        n_cols = min(3, len(columns))
        n_rows = (len(columns) + n_cols - 1) // n_cols
        
        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))
        axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]
        
        for idx, col in enumerate(columns):
            if idx < len(axes):
                ax = axes[idx]
                
                sns.boxplot(data=df, x=self.config.target_column, y=col,
                          ax=ax, palette=['#2ecc71', '#e74c3c'])
                ax.set_title(f'{col} by Fraud Status', fontsize=12, fontweight='bold')
                ax.set_xticklabels(['Non-Fraud', 'Fraud'])
        
        # Hide empty subplots
        for idx in range(len(columns), len(axes)):
            axes[idx].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.config.plots_dir, 'boxplots.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Boxplots saved")
    
    # ==================== CATEGORICAL FEATURES ANALYSIS ====================
    
    def analyze_categorical_features(self, df: pd.DataFrame) -> Dict:
        """
        Analyze categorical features.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with categorical analysis
        """
        logger.info("Analyzing categorical features...")
        
        categorical_cols = df.select_dtypes(include=['object']).columns.tolist()
        
        categorical_analysis = {}
        
        for col in categorical_cols:
            value_counts = df[col].value_counts()
            
            cat_stats = {
                "unique_values": int(df[col].nunique()),
                "top_value": str(value_counts.index[0]) if len(value_counts) > 0 else None,
                "top_frequency": int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                "value_distribution": value_counts.head(10).to_dict()
            }
            
            categorical_analysis[col] = cat_stats
        
        # Generate plots
        if categorical_cols:
            self._plot_categorical_distributions(df, categorical_cols)
        
        self.eda_results['categorical_analysis'] = categorical_analysis
        
        logger.info(f"Analyzed {len(categorical_cols)} categorical features")
        return categorical_analysis
    
    def _plot_categorical_distributions(self, df: pd.DataFrame, columns: List[str]) -> None:
        """Plot distributions for categorical features."""
        for col in columns:
            fig, axes = plt.subplots(1, 2, figsize=(14, 5))
            
            # Value counts
            ax1 = axes[0]
            value_counts = df[col].value_counts()
            bars = ax1.bar(value_counts.index, value_counts.values, color='steelblue')
            ax1.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')
            ax1.set_xlabel(col)
            ax1.set_ylabel('Count')
            ax1.tick_params(axis='x', rotation=45)
            
            # Fraud rate by category
            ax2 = axes[1]
            fraud_rate = df.groupby(col)[self.config.target_column].mean() * 100
            colors = plt.cm.RdYlGn_r(fraud_rate.values / fraud_rate.max())
            bars = ax2.bar(fraud_rate.index, fraud_rate.values, color=colors)
            ax2.set_title(f'Fraud Rate (%) by {col}', fontsize=14, fontweight='bold')
            ax2.set_xlabel(col)
            ax2.set_ylabel('Fraud Rate (%)')
            ax2.tick_params(axis='x', rotation=45)
            
            # Add percentage labels
            for bar, rate in zip(bars, fraud_rate.values):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{rate:.2f}%', ha='center', va='bottom', fontsize=10)
            
            plt.tight_layout()
            plt.savefig(os.path.join(self.config.plots_dir, f'{col}_analysis.png'),
                       dpi=300, bbox_inches='tight')
            plt.close()
        
        logger.info("Categorical distribution plots saved")
    
    # ==================== CORRELATION ANALYSIS ====================
    
    def analyze_correlations(self, df: pd.DataFrame) -> Dict:
        """
        Analyze correlations between numerical features.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with correlation analysis
        """
        logger.info("Analyzing correlations...")
        
        numerical_df = df.select_dtypes(include=[np.number])
        correlation_matrix = numerical_df.corr()
        
        # Find high correlations
        high_correlations = []
        for i in range(len(correlation_matrix.columns)):
            for j in range(i+1, len(correlation_matrix.columns)):
                corr_value = correlation_matrix.iloc[i, j]
                if abs(corr_value) > self.config.correlation_threshold:
                    high_correlations.append({
                        "feature_1": correlation_matrix.columns[i],
                        "feature_2": correlation_matrix.columns[j],
                        "correlation": round(corr_value, 3)
                    })
        
        # Correlation with target
        target_correlations = correlation_matrix[self.config.target_column].drop(
            self.config.target_column
        ).sort_values(key=abs, ascending=False)
        
        correlation_analysis = {
            "high_correlation_pairs": high_correlations,
            "target_correlations": target_correlations.round(3).to_dict(),
            "top_positive_correlations": target_correlations.head(5).to_dict(),
            "top_negative_correlations": target_correlations.tail(5).to_dict()
        }
        
        # Generate correlation heatmap
        self._plot_correlation_heatmap(correlation_matrix)
        
        logger.info(f"Found {len(high_correlations)} highly correlated feature pairs")
        logger.info(f"Top correlated with target: {list(target_correlations.head(3).index)}")
        
        self.eda_results['correlation_analysis'] = correlation_analysis
        return correlation_analysis
    
    def _plot_correlation_heatmap(self, correlation_matrix: pd.DataFrame) -> None:
        """Plot correlation heatmap."""
        fig, ax = plt.subplots(figsize=(12, 10))
        
        mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
        
        sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f',
                   cmap='RdBu_r', center=0, square=True, linewidths=0.5,
                   ax=ax, cbar_kws={"shrink": 0.8})
        
        ax.set_title('Feature Correlation Heatmap', fontsize=16, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.config.plots_dir, 'correlation_heatmap.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Correlation heatmap saved")
    
    # ==================== FRAUD PATTERN ANALYSIS ====================
    
    def analyze_fraud_patterns(self, df: pd.DataFrame) -> Dict:
        """
        Analyze patterns specific to fraudulent transactions.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with fraud pattern analysis
        """
        logger.info("Analyzing fraud patterns...")
        
        fraud_df = df[df[self.config.target_column] == 1]
        non_fraud_df = df[df[self.config.target_column] == 0]
        
        # Amount analysis
        amount_analysis = {
            "fraud": {
                "mean": round(float(fraud_df['amount'].mean()), 2),
                "median": round(float(fraud_df['amount'].median()), 2),
                "std": round(float(fraud_df['amount'].std()), 2),
                "min": round(float(fraud_df['amount'].min()), 2),
                "max": round(float(fraud_df['amount'].max()), 2)
            },
            "non_fraud": {
                "mean": round(float(non_fraud_df['amount'].mean()), 2),
                "median": round(float(non_fraud_df['amount'].median()), 2),
                "std": round(float(non_fraud_df['amount'].std()), 2),
                "min": round(float(non_fraud_df['amount'].min()), 2),
                "max": round(float(non_fraud_df['amount'].max()), 2)
            }
        }
        
        # Transaction type analysis
        if 'type' in df.columns:
            type_fraud_rate = df.groupby('type').agg({
                self.config.target_column: ['sum', 'count', 'mean']
            }).round(4)
            type_fraud_rate.columns = ['fraud_count', 'total_count', 'fraud_rate']
            type_fraud_rate['fraud_rate'] = (type_fraud_rate['fraud_rate'] * 100).round(2)
            type_fraud_rate = type_fraud_rate.sort_values('fraud_rate', ascending=False)
            
            type_analysis = type_fraud_rate.to_dict('index')
        else:
            type_analysis = {}
        
        # Balance analysis (account draining pattern)
        if 'old_balance_org' in df.columns and 'new_balance_org' in df.columns:
            # Check for account draining (common fraud pattern)
            df_temp = df.copy()
            df_temp['balance_diff'] = df_temp['old_balance_org'] - df_temp['new_balance_org']
            df_temp['is_account_drain'] = (
                (df_temp['new_balance_org'] == 0) & 
                (df_temp['old_balance_org'] > 0)
            )
            
            balance_analysis = {
                "account_drain_total": int(df_temp['is_account_drain'].sum()),
                "account_drain_fraud": int(
                    df_temp[df_temp[self.config.target_column] == 1]['is_account_drain'].sum()
                ),
                "drain_fraud_rate": round(
                    (df_temp[df_temp['is_account_drain']][self.config.target_column].mean() * 100), 2
                )
            }
        else:
            balance_analysis = {}
        
        fraud_patterns = {
            "amount_analysis": amount_analysis,
            "type_analysis": type_analysis,
            "balance_analysis": balance_analysis,
            "insights": self._generate_fraud_insights(amount_analysis, type_analysis, balance_analysis)
        }
        
        # Generate fraud pattern plots
        self._plot_fraud_patterns(df)
        
        self.eda_results['fraud_patterns'] = fraud_patterns
        return fraud_patterns
    
    def _generate_fraud_insights(
        self,
        amount_analysis: Dict,
        type_analysis: Dict,
        balance_analysis: Dict
    ) -> List[str]:
        """Generate insights from fraud patterns."""
        insights = []
        
        # Amount insights
        fraud_mean = amount_analysis['fraud']['mean']
        non_fraud_mean = amount_analysis['non_fraud']['mean']
        
        if fraud_mean > non_fraud_mean * 2:
            insights.append(
                f"Fraudulent transactions have significantly higher amounts "
                f"(avg: ${fraud_mean:,.2f}) compared to non-fraud (avg: ${non_fraud_mean:,.2f})"
            )
        
        # Type insights
        if type_analysis:
            high_risk_types = [
                t for t, stats in type_analysis.items() 
                if stats['fraud_rate'] > 1.0
            ]
            if high_risk_types:
                insights.append(
                    f"High-risk transaction types: {', '.join(high_risk_types)}"
                )
        
        # Balance insights
        if balance_analysis and balance_analysis.get('drain_fraud_rate', 0) > 10:
            insights.append(
                f"Account draining pattern detected: {balance_analysis['drain_fraud_rate']}% "
                f"of account drain transactions are fraudulent"
            )
        
        return insights
    
    def _plot_fraud_patterns(self, df: pd.DataFrame) -> None:
        """Plot fraud pattern visualizations."""
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. Amount distribution by fraud status
        ax1 = axes[0, 0]
        fraud_df = df[df[self.config.target_column] == 1]['amount']
        non_fraud_df = df[df[self.config.target_column] == 0]['amount']
        
        ax1.hist(non_fraud_df, bins=50, alpha=0.7, label='Non-Fraud', color='#2ecc71', density=True)
        ax1.hist(fraud_df, bins=50, alpha=0.7, label='Fraud', color='#e74c3c', density=True)
        ax1.set_title('Amount Distribution by Fraud Status', fontsize=14, fontweight='bold')
        ax1.set_xlabel('Amount')
        ax1.set_ylabel('Density')
        ax1.legend()
        ax1.set_xlim(0, df['amount'].quantile(0.99))  # Limit x-axis for visibility
        
        # 2. Fraud rate by transaction type
        if 'type' in df.columns:
            ax2 = axes[0, 1]
            fraud_rate = df.groupby('type')[self.config.target_column].mean() * 100
            fraud_rate = fraud_rate.sort_values(ascending=False)
            
            colors = plt.cm.RdYlGn_r(fraud_rate.values / max(fraud_rate.max(), 1))
            bars = ax2.bar(fraud_rate.index, fraud_rate.values, color=colors)
            ax2.set_title('Fraud Rate (%) by Transaction Type', fontsize=14, fontweight='bold')
            ax2.set_xlabel('Transaction Type')
            ax2.set_ylabel('Fraud Rate (%)')
            ax2.tick_params(axis='x', rotation=45)
            
            for bar, rate in zip(bars, fraud_rate.values):
                ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                        f'{rate:.2f}%', ha='center', va='bottom', fontsize=10)
        
        # 3. Transaction count by type and fraud status
        if 'type' in df.columns:
            ax3 = axes[1, 0]
            type_fraud_counts = df.groupby(['type', self.config.target_column]).size().unstack(fill_value=0)
            type_fraud_counts.plot(kind='bar', ax=ax3, color=['#2ecc71', '#e74c3c'])
            ax3.set_title('Transaction Count by Type and Fraud Status', fontsize=14, fontweight='bold')
            ax3.set_xlabel('Transaction Type')
            ax3.set_ylabel('Count')
            ax3.legend(['Non-Fraud', 'Fraud'])
            ax3.tick_params(axis='x', rotation=45)
        
        # 4. Amount percentiles comparison
        ax4 = axes[1, 1]
        percentiles = [25, 50, 75, 90, 95, 99]
        fraud_percentiles = [fraud_df.quantile(p/100) for p in percentiles]
        non_fraud_percentiles = [non_fraud_df.quantile(p/100) for p in percentiles]
        
        x = np.arange(len(percentiles))
        width = 0.35
        
        ax4.bar(x - width/2, non_fraud_percentiles, width, label='Non-Fraud', color='#2ecc71')
        ax4.bar(x + width/2, fraud_percentiles, width, label='Fraud', color='#e74c3c')
        ax4.set_title('Amount Percentiles: Fraud vs Non-Fraud', fontsize=14, fontweight='bold')
        ax4.set_xlabel('Percentile')
        ax4.set_ylabel('Amount')
        ax4.set_xticks(x)
        ax4.set_xticklabels([f'{p}th' for p in percentiles])
        ax4.legend()
        
        plt.tight_layout()
        plt.savefig(os.path.join(self.config.plots_dir, 'fraud_patterns.png'),
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        logger.info("Fraud pattern plots saved")
    
    # ==================== OUTLIER DETECTION ====================
    
    def detect_outliers(self, df: pd.DataFrame) -> Dict:
        """
        Detect outliers in numerical features.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with outlier analysis
        """
        logger.info("Detecting outliers...")
        
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if self.config.target_column in numerical_cols:
            numerical_cols.remove(self.config.target_column)
        
        outlier_analysis = {}
        outlier_columns = []
        
        for col in numerical_cols:
            # Z-score method
            z_scores = np.abs(stats.zscore(df[col].dropna()))
            z_outliers = (z_scores > self.config.outlier_std_threshold).sum()
            
            # IQR method
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR
            iqr_outliers = ((df[col] < lower_bound) | (df[col] > upper_bound)).sum()
            
            outlier_pct = (iqr_outliers / len(df)) * 100
            
            outlier_analysis[col] = {
                "z_score_outliers": int(z_outliers),
                "iqr_outliers": int(iqr_outliers),
                "outlier_percentage": round(outlier_pct, 2),
                "lower_bound": round(lower_bound, 2),
                "upper_bound": round(upper_bound, 2)
            }
            
            if outlier_pct > 5:  # More than 5% outliers
                outlier_columns.append(col)
        
        result = {
            "outlier_analysis": outlier_analysis,
            "columns_with_high_outliers": outlier_columns
        }
        
        logger.info(f"Columns with high outliers (>5%): {outlier_columns}")
        
        self.eda_results['outlier_analysis'] = result
        return result
    
    # ==================== STATISTICAL TESTS ====================
    
    def perform_statistical_tests(self, df: pd.DataFrame) -> Dict:
        """
        Perform statistical tests to compare fraud vs non-fraud.
        
        Args:
            df: DataFrame to analyze
        
        Returns:
            Dict with statistical test results
        """
        logger.info("Performing statistical tests...")
        
        numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if self.config.target_column in numerical_cols:
            numerical_cols.remove(self.config.target_column)
        
        fraud_df = df[df[self.config.target_column] == 1]
        non_fraud_df = df[df[self.config.target_column] == 0]
        
        test_results = {}
        
        for col in numerical_cols:
            # Mann-Whitney U test (non-parametric)
            try:
                stat, p_value = stats.mannwhitneyu(
                    fraud_df[col].dropna(),
                    non_fraud_df[col].dropna(),
                    alternative='two-sided'
                )
                
                test_results[col] = {
                    "test": "Mann-Whitney U",
                    "statistic": round(float(stat), 4),
                    "p_value": round(float(p_value), 6),
                    "significant": p_value < 0.05,
                    "interpretation": "Significant difference" if p_value < 0.05 else "No significant difference"
                }
            except Exception as e:
                test_results[col] = {
                    "test": "Mann-Whitney U",
                    "error": str(e)
                }
        
        # Find most significant features
        significant_features = [
            col for col, result in test_results.items()
            if result.get('significant', False)
        ]
        
        result = {
            "test_results": test_results,
            "significant_features": significant_features,
            "significance_count": len(significant_features)
        }
        
        logger.info(f"Features with significant difference: {len(significant_features)}/{len(numerical_cols)}")
        
        self.eda_results['statistical_tests'] = result
        return result
    
    # ==================== SUMMARY & RECOMMENDATIONS ====================
    
    def generate_summary_and_recommendations(self) -> Dict:
        """
        Generate EDA summary and recommendations.
        
        Returns:
            Dict with summary and recommendations
        """
        logger.info("Generating summary and recommendations...")
        
        summary = {
            "data_quality": {
                "missing_values": self.eda_results.get('overview', {}).get('missing_values', {}).get('total', 0),
                "duplicates": self.eda_results.get('overview', {}).get('duplicates', {}).get('count', 0),
                "quality_score": "Good" if self.eda_results.get('overview', {}).get('missing_values', {}).get('total', 0) == 0 else "Needs Attention"
            },
            "class_imbalance": self.eda_results.get('target_analysis', {}),
            "key_findings": [],
            "recommendations": []
        }
        
        # Add key findings
        fraud_pct = self.eda_results.get('target_analysis', {}).get('fraud_percentage', 0)
        if fraud_pct < 2:
            summary['key_findings'].append(
                f"Severe class imbalance detected ({fraud_pct}% fraud). "
                "SMOTE or other resampling techniques recommended."
            )
        
        # Correlation findings
        high_corr = self.eda_results.get('correlation_analysis', {}).get('high_correlation_pairs', [])
        if high_corr:
            summary['key_findings'].append(
                f"Found {len(high_corr)} highly correlated feature pairs. "
                "Consider removing redundant features."
            )
        
        # Fraud pattern findings
        fraud_patterns = self.eda_results.get('fraud_patterns', {})
        if fraud_patterns.get('insights'):
            summary['key_findings'].extend(fraud_patterns['insights'])
        
        # Recommendations
        summary['recommendations'] = [
            "Apply SMOTE/ADASYN to handle class imbalance",
            "Use stratified sampling for cross-validation",
            "Consider ensemble methods (XGBoost, LightGBM) for imbalanced data",
            "Focus on recall metric for fraud detection",
            "Create transaction velocity features",
            "Engineer balance difference features",
            "Apply appropriate scaling for numerical features",
            "Use threshold tuning to optimize precision-recall tradeoff"
        ]
        
        self.eda_results['summary'] = summary
        return summary
    
    # ==================== MAIN EXECUTION ====================
    
    def initiate_eda(self) -> EDAArtifact:
        """
        Execute complete EDA pipeline.
        
        Returns:
            EDAArtifact: EDA results artifact
        """
        logger.info(f"{'='*60}")
        logger.info("Starting Exploratory Data Analysis")
        logger.info(f"{'='*60}")
        
        try:
            # Step 1: Create directories
            logger.info("Step 1: Creating directories")
            create_directories([self.config.eda_dir, self.config.plots_dir])
            
            # Step 2: Load data
            logger.info("Step 2: Loading data")
            df = self._load_data()
            
            # Step 3: Data overview
            logger.info("Step 3: Analyzing data overview")
            overview = self.analyze_data_overview(df)
            
            # Step 4: Target analysis
            logger.info("Step 4: Analyzing target distribution")
            target_analysis = self.analyze_target_distribution(df)
            
            # Step 5: Numerical features
            logger.info("Step 5: Analyzing numerical features")
            numerical_analysis = self.analyze_numerical_features(df)
            
            # Step 6: Categorical features
            logger.info("Step 6: Analyzing categorical features")
            categorical_analysis = self.analyze_categorical_features(df)
            
            # Step 7: Correlation analysis
            logger.info("Step 7: Analyzing correlations")
            correlation_analysis = self.analyze_correlations(df)
            
            # Step 8: Fraud patterns
            logger.info("Step 8: Analyzing fraud patterns")
            fraud_patterns = self.analyze_fraud_patterns(df)
            
            # Step 9: Outlier detection
            logger.info("Step 9: Detecting outliers")
            outlier_analysis = self.detect_outliers(df)
            
            # Step 10: Statistical tests
            logger.info("Step 10: Performing statistical tests")
            statistical_tests = self.perform_statistical_tests(df)
            
            # Step 11: Summary and recommendations
            logger.info("Step 11: Generating summary")
            summary = self.generate_summary_and_recommendations()
            
            # Step 12: Save EDA report
            logger.info("Step 12: Saving EDA report")
            write_json(self.config.report_path, self.eda_results)
            
            # Create artifact
            artifact = EDAArtifact(
                eda_report_path=self.config.report_path,
                plots_dir=self.config.plots_dir,
                total_records=overview['shape']['rows'],
                total_features=overview['shape']['columns'],
                numerical_features=overview['features']['numerical_list'],
                categorical_features=overview['features']['categorical_list'],
                fraud_count=target_analysis['fraud_count'],
                non_fraud_count=target_analysis['non_fraud_count'],
                fraud_percentage=target_analysis['fraud_percentage'],
                imbalance_ratio=target_analysis['imbalance_ratio'],
                high_correlation_pairs=[
                    (p['feature_1'], p['feature_2'], p['correlation'])
                    for p in correlation_analysis['high_correlation_pairs']
                ],
                outlier_columns=outlier_analysis['columns_with_high_outliers']
            )
            
            logger.info(f"{'='*60}")
            logger.info("EDA COMPLETED SUCCESSFULLY!")
            logger.info(f"{'='*60}")
            logger.info(f"Report saved: {self.config.report_path}")
            logger.info(f"Plots saved: {self.config.plots_dir}")
            
            return artifact
            
        except Exception as e:
            raise FraudDetectionException(
                error_message=f"EDA failed: {str(e)}",
                error_detail=sys
            ) from e


# ============== STANDALONE EXECUTION ==============
if __name__ == "__main__":
    from src.entity.config_entity import DataIngestionConfig
    from src.components.data_ingestion import DataIngestion
    
    # Run Data Ingestion first
    ingestion_config = DataIngestionConfig(
        source_type="csv",
        source_path="data/raw/transactions.csv"
    )
    
    ingestion = DataIngestion(config=ingestion_config)
    ingestion_artifact = ingestion.initiate_data_ingestion()
    
    # Run EDA
    eda_config = EDAConfig()
    eda = DataEDA(
        config=eda_config,
        data_ingestion_artifact=ingestion_artifact
    )
    
    eda_artifact = eda.initiate_eda()
    
    print("\n" + "="*60)
    print("EDA ARTIFACT SUMMARY")
    print("="*60)
    print(f"Report Path: {eda_artifact.eda_report_path}")
    print(f"Plots Directory: {eda_artifact.plots_dir}")
    print(f"Total Records: {eda_artifact.total_records:,}")
    print(f"Fraud Percentage: {eda_artifact.fraud_percentage}%")
    print(f"Imbalance Ratio: 1:{eda_artifact.imbalance_ratio}")
    print(f"High Correlation Pairs: {len(eda_artifact.high_correlation_pairs)}")
    print(f"Outlier Columns: {eda_artifact.outlier_columns}")